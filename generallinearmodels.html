<!DOCTYPE HTML>
<html>
    <head>
        <title>General Linear Models</title>

        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.scrolly.min.js"></script>
        <script src="js/jquery.scrollgress.min.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-layers.min.js"></script>
        <script src="js/init.js"></script>
        <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

        <noscript>
            <link rel="stylesheet" href="css/skel.css" />
            <link rel="stylesheet" href="css/style.css" />
            <link rel="stylesheet" href="css/style-wide.css" />
            <link rel="stylesheet" href="css/style-noscript.css" />
        </noscript>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
        <script src="js/smoothscroll.js"></script>

        <link rel="stylesheet" type="text/css" media="screen"
        href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.css" />
        <style type="text/css">
            a.fancybox img {
                border: none;
            }
        </style>

        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-68428630-1', 'auto');
            ga('send', 'pageview');

        </script>

    </head>


    <body class="index">

       <!-- Header -->
        <header id="header" style="position: fixed; top: 0px; left: 0px; right: 0px; z-index: 1030;">
            <h1 id="logo"><a href="index.html">Home</a></h1>
            <nav id="nav">
                <ul>
                    <li><a href="#top" class="smoothScroll">^</a></li>
                    <li><a href="#contact" class="smoothScroll">Contact</a></li>
                </ul>
            </nav>
        </header>

        <!-- Main -->
        <article id="main">

            <!-- Emoji -->
            <div id="top">
                <section class="wrapper style3 container">

                    <header>
                        <h2><center>General Linear Models</center></h2>
                    </header>

                    <!-- Text -->
                    <p>
                        A <em>general linear model</em> describes a dependent
                        variable in terms of a linear combination of independent
                        variables and an error term.
                    </p>
                    <h3>Simple Linear Regression</h3>
                    <p>
                        A line can be described by the equation \(y = mx + b\),
                        where \(b\) is the \(y\)-intercept and \(m\) is the
                        slope. Simple linear regression uses this idea to
                        describe the <em>relationship</em> between two
                        variables&mdash;one dependent, the \(y\), and one
                        independent, the \(x\). Dependent variables are also
                        sometimes referred to as "response variables,"
                        "regressands," or "targets." The corresponding terms for
                        independent variables are "predictor variables,"
                        "regressors," or "features."
                    </p>
                    <p>
                        The objective is to specify a model that explains \(y\)
                        in terms of \(x\) in the <em>population</em> of
                        interest. A simple model can be specified as follows.
                    </p>
                    <p>
                        \[y = \beta_0 + \beta_1 x + \epsilon\]
                    </p>
                    <p>
                        In this specification&mdash;called the scalar
                        form&mdash;\(\beta_0\) represents the \(y\)-intercept
                        and \(\beta_1\) represents the slope. The \(\epsilon\),
                        which is called the error term, represents
                        <em>other</em> factors that affect \(y\). If the error
                        term is excluded, the implication is that the
                        relationship is exact or <em>deterministic</em>.
                        On the other hand, if we include the error term, we
                        imply that the relationship also contains a
                        <em>stochastic</em> component. For most, if not all,
                        applications of regression, the relationship between
                        the dependent and independent variables includes both
                        deterministic and stochastic parts.
                    </p>
                    <p>
                        The actual \(\beta\) values in the population are
                        unknown. However, they can be estimated using
                        data&mdash;preferably a random sample from the
                        population. We use this data to find the line that
                        "best" fits it. Because each observation in our sample
                        has its own values, we use subscripts to denote them.
                    </p>
                    <p>
                        \[y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]
                    </p>
                    <p>
                        Notice that the \(y\), \(x\), and \(\epsilon\) terms
                        have subscripts, but the \(\beta\)s do not. This implies
                        that there is a <em>single</em> \(y\)-intercept and
                        slope&mdash;the one from the population equation that
                        represents the "true" line.
                    </p>

                    <h3>Multiple Regression</h3>
                    <p>
                        Often, outcomes for our variable of interest, \(y\),
                        depend on two or more predictors. This is referred to as
                        a <em>multiple</em> linear regression model, which, for
                        the case of two variables, can be written as:
                    </p>
                    <p>
                        \[y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i\]
                    </p>
                    <p>
                        As you might imagine, the scalar form gets unwieldy as
                        the number of independent variables increases. To deal
                        with this, we introduce an alternative way to express
                        our model.
                    </p>
                    <h3>Matrix Form</h3>
                    <p>
                        While the scalar form allows us to be explicit about
                        the regressors included in the model, it is often more
                        efficient to write the equation in matrix form. To
                        motivate the reasons why, consider the case where, for
                        our equation above, \(i = 1, 2, \ldots,n\).
                    </p>
                    <p>
                        \[y_1 = \beta_0 + \beta_1 x_{11} + \beta_2 x_{21} + \epsilon_1\]
                        \[y_2 = \beta_0 + \beta_1 x_{12} + \beta_2 x_{22} + \epsilon_2\]
                        \[\vdots\]
                        \[y_n = \beta_0 + \beta_1 x_{1n} + \beta_2 x_{2n} + \epsilon_n\]
                    </p>
                    <p>
                        Our <em>system</em> of \(n\) equations can, instead, be
                        represented in terms of matrix addition. Note that we've
                        included a vector of ones of length \(n\), which we'll
                        call \(x_0\), that we multiply with \(\beta_0\).
                    </p>
                    <p style="font-size: 85%">
                        \begin{split}
                            \left[\begin{matrix}y_1\\ \vdots\\ y_n\end{matrix}\right] =
                            \beta_0 \left[\begin{matrix}1 \\ \vdots\\ 1\end{matrix}\right] +
                            \beta_1 \left[\begin{matrix}x_{11}\\ \vdots\\ x_{1n}\end{matrix}\right] +
                            \beta_2 \left[\begin{matrix}x_{21}\\ \vdots\\ x_{2n}\end{matrix}\right] +
                            \left[\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right]
                        \end{split}
                    </p>
                    <p>
                        This can be restated in terms of matrix multiplication.
                    </p>
                    <p style="font-size: 85%">
                        \begin{split}
                            \left[\begin{matrix}y_1\\ \vdots\\ y_n\end{matrix}\right] =
                            \left[
                                \begin{matrix}
                                    1 & x_{11} & x_{21}\\
                                    \vdots & \vdots & \vdots\\
                                    1 & x_{1n} & x_{2n}
                                \end{matrix}
                            \right]
                            \left[\begin{matrix}\beta_0\\ \beta_1\\ \beta_2\end{matrix}\right] +
                            \left[\begin{matrix}\epsilon_1\\ \vdots\\ \epsilon_n\end{matrix}\right]
                        \end{split}
                    </p>
                    <p>
                        Here, we've simply combined the \(x_0\), \(x_1\), and
                        \(x_2\) into a matrix, which we refer to as \(X\),
                        called the <em>design matrix</em>, and
                        combined the \(\beta\)s into a column vector, which
                        we refer to as \(\vec{\beta}\), called the <em>parameter
                        vector</em>. In this example, \(X\) is an
                        \(n \times p\) matrix and the column vector
                        \(\vec{\beta}\) can be thought of as a \(p \times 1\)
                        matrix, where \(p = 3\). Because the number of
                        columns in \(X\) equals the number of rows in
                        \(\vec{\beta}\), they can be multiplied, resulting in an
                        \(n \times 1\) column vector. This is the deterministic
                        portion of our model, which, when added to the
                        stochastic component, \(\vec{\epsilon}\), results in
                        \(\vec{y}\).
                    </p>
                    <p>
                        The matrix form of our model can be rewritten compactly
                        using vector notation.
                    </p>
                    <p>
                        \[\vec{y} = X\vec{\beta} + \vec{\epsilon}\]
                    </p>
                    </p>
                        This specification can handle an arbitrary number of
                        parameters. For each parameter, \(p\), add a column to
                        \(X\) and an element to \(\vec{\beta}\).
                    </p>
                    <h3>Solving</h3>
                    <p>
                        There are several ways to solve for the "best" line. To
                        illustrate this, we'll go back to the simple linear
                        regression model. Let's suppose we'd like to create a
                        regression line for the data shown in the following
                        plot, created using the code below. (Note: the complete
                        set of commands used to make this plot can be found
                        <a href="https://gist.github.com/juanshishido/e14209aaa26b96b756ee#file-slr_data-py"
                        target="_blank">here</a>.)
                    </p>
<pre>
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1868)
x = np.random.normal(0, 1, 20)
y = np.array([np.random.normal(v, 0.5) for v in x])

plt.scatter(x, y, s=35, alpha=0.85, color='#328E82')
</pre>
                    <p>
                        <center>
                            <img src="images/slr-data.png" class="fancybox"
                            style="border:1px solid lightgray; width:90%;">
                        </center>
                    </p>
                    <p>
                        The "best" line is usually thought of as the one that
                        <em>minimizes</em> the vertical distance between it and
                        the data points. Typically, the distance that is
                        minimized is the sum of the squared residuals. This is
                        the ordinary least squares estimation. Residuals are the
                        difference between each \(y_i\) and its estimated (or
                        fitted) value, \(\hat{y}_i\). In vector notation, these
                        can be written as \(\vec{y}\) and \(\hat{\vec{y}}\),
                        respectively. The idea is to find values of \(\beta_0\)
                        and \(\beta_1\) that minimize:
                        \[\sum_{i=1}^n (y_i - \hat{y}_i)^2\]
                    </p>
                    <p>
                        The terms "errors" and "residuals" are often used as
                        synonyms, unfortunately. In reality, they are quite
                        distinct. Errors are a population-level concept and are
                        unobservable. Residuals, on the other hand, are
                        sample-specific and are computed from the data when we
                        estimate the \(\beta\)s.
                    </p>
                    <p>
                        Recall that we can specify the model for the data above
                        as \(\vec{y} = X\vec{\beta} + \vec{\epsilon}\). From
                        this, we can use matrix algebra to estimate the "best"
                        line.
                    </p>
                    <p>
                        \[\hat{\vec{\beta}} = (X^T X)^{-1} X^T \vec{y}\]
                    </p>
                    <p>
                        Using NumPy, we can estimate the parameter vector as
                        follows.
                    </p>
<pre>
import numpy.linalg as npl

ones = np.ones(len(x))
X = np.column_stack((ones, x))
b = npl.inv(X.T.dot(X)).dot(X.T).dot(y)

</pre>
                    <p>
                        Here, \(b\) is the estimated \(\hat{\vec{\beta}}\) with
                        values <font face="courier new">[-0.08396926,
                        1.10769145]</font>. These represent \(\hat{\beta}_0\)
                        and \(\hat{\beta}_1\), respectively. This results in the
                        following regression line, in which is the sum of the
                        squared distances between the data points and the line
                        have been minimized.
                    </p>
                    <p>
                        <center>
                            <img src="images/slr-line.png" class="fancybox"
                            style="border:1px solid lightgray; width:90%;">
                        </center>
                    </p>
                    <p>
                        For this approach to work, \(X^T X\) must be invertible
                        or nonsingular. This means that the columns in \(X\)
                        must be linearly <em>independent</em>. If this isn't the
                        case&mdash;that is, if \(X^T X\) is <em>not</em>
                        invertible&mdash;then there will be an infinite number
                        of solutions for \(\hat{\vec{\beta}}\).
                    </p>
                    <h3>The General Linear Model</h3>
                    <p>
                        "The term <em>general linear model</em> refers to a
                        linear model of form \(\vec{y} = X\vec{\beta} +
                        \vec{\epsilon}\)" (Poline and Brett). It's "general" in
                        the sense that the data in \(X\) can be continuous,
                        discrete, or even binary (dummy). The errors,
                        \(\vec{\epsilon}\), are assumed to be independent and
                        identically distributed with a zero mean and
                        \(\sigma^2\) variance. General linear models need not
                        only describe a single \(y\). That is, they can also
                        cover multivariate models.
                    </p>
                    <h3>Final Thoughts</h3>
                    <p>
                        We've seen how to construct simple and multiple linear
                        regression models using matrix notation and how to
                        estimate the parameters using NumPy. These models are
                        useful and are used often for describing the
                        relationships between variables.
                    </p>
                    <p>
                        The following sources were invaluable for my
                        understanding of these concepts.
                        <font size="2">
                            <ol>
                                <li>
                                    Millman, Jarrod. <a href=
                                    "https://www.jarrodmillman.com/rcsds/lectures/glm_intro.html"
                                    target="_blank">"Introduction to the general
                                    linear model"</a>
                                </li>
                                <li>
                                    Kiebel, S.J and A.P. Holmes. <a href=
                                    "https://www.fil.ion.ucl.ac.uk/spm/doc/books/hbf2/pdfs/Ch7.pdf"
                                    target="_blank">"The general linear model"</a>
                                </li>
                                <li>
                                    Penn State, STAT 501. <a href=
                                    "https://onlinecourses.science.psu.edu/stat501/node/382"
                                    target="_blank">"A Matrix Formulation of the
                                    Multiple Regression Model"</a>
                                </li>
                                <li>
                                    Poline, Jean-Baptiste and Matthew Brett. <a
                                    href="https://matthew.dynevor.org/_downloads/does_glm_love.pdf"
                                    target="_blank">"The General Linear Model
                                    and fMRI: does love last forever?"</a>
                                </li>
                                <li>
                                    Wooldridge, Jeffrey M. <a href=
                                    "https://www.amazon.com/Introductory-Econometrics-Modern-Approach-Economics/dp/1111531048"
                                    target="_blank">"Introductory
                                    Econometrics: A Modern Approach"</a>
                                </li>
                            </ol>
                        </font>
                    </p>

                </section>
            </div>
        </article>

        <!-- Footer -->
        <div id="contact">
            <footer id="footer">

                <h2><strong>Contact</strong></h2>
                <ul class="icons">
                    <li>
                        <a href="https://www.linkedin.com/in/juanshishido/"
                        class="icon circle fa-linkedin" target="_blank">
                        <span class="label">LinkedIn</span></a>
                    </li>
                    <li>
                        <a href="https://twitter.com/juanshishido"
                        class="icon circle fa-twitter" target="_blank">
                        <span class="label">Twitter</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/juanshishido"
                        class="icon circle fa-github" target="_blank">
                        <span class="label">Github</span></a>
                    </li>
                </ul>

                <ul class="copyright">
                    <li>
                        <a href="index.html" class="smoothScroll">Juan Shishido</a>
                    </li>
                    <li>Design:
                        <a href="https://html5up.net" target="_blank">HTML5 UP</a>
                    </li>
                </ul>

            </footer>
        </div>

        <script type="text/javascript"
        src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
        <script type="text/javascript"
        src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
        <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.pack.min.js"></script>
        <script type="text/javascript">
            $(function($){
                var addToAll = false;
                var gallery = false;
                var titlePosition = 'over';
                $(addToAll ? 'img' : 'img.fancybox').each(function(){
                    var $this = $(this);
                    var title = $this.attr('title');
                    var src = $this.attr('data-big') || $this.attr('src');
                    var a = $('<a href="#" class="fancybox"></a>').attr('href', src).attr('title', title);
                    $this.wrap(a);
                });
                if (gallery)
                    $('a.fancybox').attr('rel', 'fancyboxgallery');
                $('a.fancybox').fancybox({
                    titlePosition: titlePosition
                });
            });
            $.noConflict();
        </script>

    </body>
</html>
